# Language Translation Model by Stabya Acharya st124150

### Datasets 
#### 
Dataset for this model has been used using `opus100` available through the Hugging Face `datasets` library.
### Demo 
![demofinal](https://github.com/stabyaaaa/Natural-Language-Processing/assets/35591848/39d6089d-c1fd-4236-bb1f-f7f91ebd698e)

### PREPROCESSING STEPS
Random Sampling: Select a subset of training data.
Tokenization: Break text into smaller units.
Normalization: Standardize text format.
Word Segmentation: Segment text for languages without spaces.
Mapping Tokenization Function: Apply tokenization function to the dataset.

`numpy.random` for random sampling.
`torchtext.data.utils` for tokenization.
`nepalitokenizers for` Nepali tokenization.


### Task 2: Experiment with Attentions Mechanisms
1. General Attention
2. Multiplicative Attention
3. Additive Attention


| Attention Variant | Training Loss | Training PPL | Validation Loss | Validation PPL   |
|-------------------|---------------|--------------|------------------|-----------------|
| General           | 6.118         | 454.087      | 5.43             |  228.504        |
| Multiplicative    | 6.84          | 933.699      | 5.99             | 398.01          |
| Additive          | 6.62          | 753.33       | 5.81             | 332.82          |

### Comparision Betweeen Attentions | Results

#### General Attention

Achieved the lowest training and validation losses, indicating better performance in learning the translation task.
Lower perplexity values suggest that the model generated translations with higher fluency and coherence.
The general attention mechanism is designed to capture dependencies between input and output sequences effectively, which likely contributed to its superior performance.
Overall, the general attention mechanism appears to be highly effective in translating between the native language and English based on the provided metrics.

#### Multiplicative Attention

Showed higher training and validation losses compared to the general attention mechanism.
Higher perplexity values suggest that the translations may be less fluent and coherent compared to those generated by the general attention mechanism.
The multiplicative attention mechanism may not have effectively captured the relationships between input and output sequences, leading to poorer performance in translation.

#### Additive Attention

Demonstrated intermediate results between general and multiplicative attention.
While it performed better than multiplicative attention, it still had slightly higher training and validation losses than the general attention mechanism.
The additive attention mechanism attempts to capture dependencies through a different approach, but it may not have been as effective as the general attention mechanism in this specific translation task.

In conclusion, the general attention mechanism appears to be the most effective in translating between the Nepali language and English. Its ability to capture dependencies between input and output sequences led to lower training and validation losses, as well as lower perplexity values, indicating higher fluency and coherence in the generated translations. However, further analysis and evaluation, including qualitative assessment and testing on diverse datasets, may be necessary to fully understand the effectiveness of each attention mechanism in this translation task.





### Plots of Attention models
#### 1. General attention
![g](https://github.com/stabyaaaa/Natural-Language-Processing/assets/35591848/e728b762-632b-4110-8536-496a96336946)
### 2. Multiplicative Attention
![m](https://github.com/stabyaaaa/Natural-Language-Processing/assets/35591848/16bb1543-5316-4a2c-9535-4f759215ea31)
### 3. Addtitive Attention
![a](https://github.com/stabyaaaa/Natural-Language-Processing/assets/35591848/5f06dc1b-60f1-42bc-b990-fc2a753625ce)

## Web Application
To access the web application for the translation model, you can run the `main.py` file. Once the Flask server is up and running, you can access the application by navigating to `localhost:5000` in your web browser.

Make sure to update the necessary configurations, such as host and port settings, in the main.py file if you want to run the application on a different address or port.

You can start the Flask server by executing the following command in your terminal:
`python main.py`


