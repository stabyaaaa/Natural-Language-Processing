{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5896b1a6-8ed4-4484-96e5-50d0ac10b73a",
   "metadata": {},
   "source": [
    "# [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)\n",
    "\n",
    "Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "[Python Script](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f636b5-91b3-4a45-a6cf-334425eac4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install peft==0.7.1\n",
    "# !pip3 install trl==0.7.4\n",
    "# !pip3 install transformer==4.36.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd24274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7263f867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74ed1948-2b9b-4324-ba26-36b6c95fdbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16f67f1-c0e4-40e9-b192-4d1a9cfbfb17",
   "metadata": {},
   "source": [
    "## Instruction-Tuning\n",
    "Train on completions only\n",
    "- Use the DataCollatorForCompletionOnlyLM to train your model on the generated prompts only.\n",
    "- Note that this works only in the case when packing=False.\n",
    "- To instantiate that collator for instruction data, pass a response template and the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1ea5ec-482c-4520-bd97-3ccc1f2961f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset_train = load_dataset('json', data_files='dataset/alpaca_data.json', split='train')\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b6ee677-ddc8-4a81-9d86-9219de3d84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '(A musical note)',\n",
       " 'output': 'The musical note is an F sharp.',\n",
       " 'instruction': 'Name the given musical note.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9b7c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.10k/8.10k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 30.0/30.0 [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 621k/621k [00:01<00:00, 619kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\.cache\\huggingface\\datasets\\downloads\\07bde58ae497102ab81d326d84eafcf6c2c7e8df8cd8b8d0ef64d9eceab41ada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating eval split: 805 examples [00:00, 33777.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output'],\n",
       "    num_rows: 805\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_eval = load_dataset(\"tatsu-lab/alpaca_eval\", split='eval', trust_remote_code=True)\n",
    "dataset_eval = dataset_eval.remove_columns([\"generator\", \"dataset\"])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8433703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'what are five important topics for game design',\n",
       " 'output': '1. Storytelling\\n2. Player Mechanics\\n3. Art Direction\\n4. Level Design\\n5. User Interface Design'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_eval[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69100196-d9d8-4791-9e11-6e93f1bd7550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 762/762 [00:00<?, ?B/s] \n",
      "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Acer\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "model.safetensors: 100%|██████████| 353M/353M [00:14<00:00, 24.1MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 124kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 25.9kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 885kB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 552kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.38MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load the model & Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name_or_path = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_prompts_func(example):\n",
    "#     output_texts = []\n",
    "#     for i in range(len(example['instruction'])):\n",
    "#         text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "#         output_texts.append(text)\n",
    "#     return output_texts\n",
    "\n",
    "# #check instruction-prompt\n",
    "# formatting_prompts_func(dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ca418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Define the Trainer\n",
    "# trainer = SFTTrainer(\n",
    "#     model,\n",
    "#     train_dataset=dataset.select(range(1000)),\n",
    "#     formatting_func=formatting_prompts_func,\n",
    "#     data_collator=collator,\n",
    "# )\n",
    "\n",
    "# trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d301f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['instruction', 'output'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_eval[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdc0605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ['', ''],\n",
       " 'output': ['1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       "  'The three primary colors are red, blue, and yellow.'],\n",
       " 'instruction': ['Give three tips for staying healthy.',\n",
       "  'What are the three primary colors?']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75210c9",
   "metadata": {},
   "source": [
    "### Standard-Alpaca : Format your input prompts\n",
    "For instruction fine-tuning, it is quite common to have two columns inside the dataset: one for the prompt & the other for the response.\n",
    "\n",
    "This allows people to format examples like Stanford-Alpaca did as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = '''\n",
    "# Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# {instruction}\n",
    "\n",
    "# ### Response:\n",
    "# {response}\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d350a2-002b-40e2-8c10-9afea5923cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    output_texts = []\n",
    "\n",
    "    for i in range(len(examples['instruction'])):\n",
    "        instruction = examples[\"instruction\"][i]\n",
    "        input_text = examples[\"input\"][i] if 'input' in examples.keys() else \"\"\n",
    "        response = examples[\"output\"][i]\n",
    "        \n",
    "        if len(input_text) > 1:\n",
    "            text = f\"\"\"\n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "            # Instruction: {instruction}\n",
    "            # Input:\n",
    "            {input_text}\n",
    "\n",
    "            # Response:\n",
    "            {response}\n",
    "\n",
    "            # Your task: Modify the given code.\n",
    "            \"\"\"\n",
    "            output_texts.append(text)\n",
    "    \n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44cf1dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForCompletionOnlyLM(tokenizer=GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the DataCollatorForCompletionOnlyLM to train your model on the generated prompts only\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"### Response:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2274f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28385087-8eb8-4b83-a7dd-1313bf591b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7006.07 examples/s]\n",
      "Map: 100%|██████████| 805/805 [00:00<00:00, 3473.73 examples/s]\n",
      "  0%|          | 0/15000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 500/15000 [02:12<58:38,  4.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6654, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1000/15000 [04:15<1:01:29,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6308, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1500/15000 [06:19<1:00:36,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5518, 'learning_rate': 4.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2000/15000 [08:23<45:53,  4.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5176, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2501/15000 [10:38<52:16,  3.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4691, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3000/15000 [12:45<50:36,  3.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5089, 'learning_rate': 4e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3500/15000 [14:52<44:31,  4.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4718, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4000/15000 [17:07<41:47,  4.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4342, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 4501/15000 [19:19<35:36,  4.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4213, 'learning_rate': 3.5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5000/15000 [21:28<45:22,  3.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4404, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 5000/15000 [22:32<45:22,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.266629457473755, 'eval_runtime': 63.4956, 'eval_samples_per_second': 12.678, 'eval_steps_per_second': 6.347, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 5500/15000 [24:39<34:09,  4.64it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1132, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6000/15000 [26:56<32:49,  4.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1395, 'learning_rate': 3e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6500/15000 [29:09<30:37,  4.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1145, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7001/15000 [31:21<26:49,  4.97it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.114, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7501/15000 [33:28<27:19,  4.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1527, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8000/15000 [35:38<24:46,  4.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.072, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8501/15000 [37:48<23:32,  4.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1227, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9000/15000 [39:56<26:47,  3.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1047, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 9500/15000 [42:01<20:04,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1137, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10000/15000 [44:06<11:08,  7.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1251, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|██████▋   | 10000/15000 [45:07<11:08,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.264587879180908, 'eval_runtime': 60.7449, 'eval_samples_per_second': 13.252, 'eval_steps_per_second': 6.634, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 10500/15000 [47:19<17:13,  4.35it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9179, 'learning_rate': 1.5e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11000/15000 [49:19<12:29,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9209, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 11500/15000 [51:25<13:12,  4.41it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9093, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12001/15000 [53:31<11:25,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8931, 'learning_rate': 1e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 12501/15000 [55:31<09:34,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.954, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13001/15000 [57:39<08:42,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9601, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 13501/15000 [59:42<05:16,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9034, 'learning_rate': 5e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14001/15000 [1:01:43<03:40,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8551, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 14501/15000 [1:03:42<01:38,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9018, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [1:05:43<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9002, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|██████████| 15000/15000 [1:06:43<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.279604911804199, 'eval_runtime': 60.2418, 'eval_samples_per_second': 13.363, 'eval_steps_per_second': 6.69, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [1:06:45<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4005.1725, 'train_samples_per_second': 7.49, 'train_steps_per_second': 3.745, 'train_loss': 2.179969942220052, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=2.179969942220052, metrics={'train_runtime': 4005.1725, 'train_samples_per_second': 7.49, 'train_steps_per_second': 3.745, 'train_loss': 2.179969942220052, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./training_results',  # Specify the output directory\n",
    "    save_strategy='epoch',  # Save model checkpoints every epoch\n",
    "    evaluation_strategy='epoch',  # Evaluate every epoch\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    per_device_train_batch_size=2,  # Batch size for training\n",
    "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model,  # Pass the model\n",
    "    args=training_args,  # Pass the training arguments\n",
    "    train_dataset=dataset_train.select(range(10000)),  # Train dataset\n",
    "    eval_dataset=dataset_eval,  # Evaluation dataset\n",
    "    formatting_func=formatting_prompts_func,  # Custom formatting function\n",
    "    data_collator=collator,  # Data collator\n",
    "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "trainer.save_model('model/instruction_tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f38791",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5da45651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name or path\n",
    "model = \"model/instruction_tuning\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model, device_map='auto'\n",
    ")\n",
    "\n",
    "# Import the pipeline for text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define a text generation pipeline with the loaded model and tokenizer\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500  # Maximum number of tokens to generate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9963e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_prompt(instruction, prompt_input=None):\n",
    "    # If prompt_input is provided\n",
    "    if prompt_input:\n",
    "        # Generate instruction prompt with input\n",
    "        return f\"\"\"\n",
    "        Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {instruction}  # Display instruction\n",
    "\n",
    "        ### Input:\n",
    "        {prompt_input}  # Display input\n",
    "\n",
    "        ### Response:  # Prompt for response\n",
    "        \"\"\".strip()\n",
    "    else:\n",
    "        # Generate instruction prompt without input\n",
    "        return f\"\"\"\n",
    "        Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {instruction}  # Display instruction\n",
    "\n",
    "        ### Response:  # Prompt for response\n",
    "        \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe70c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{What do alpacas eat?, Alpacas primarily eat grass and hay, as well as grains and supplements in captivity.}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset_eval[189]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9da7f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset is a collection of information and data related to alpacas, a species of domesticated South American camelids. It contains various attributes such as alpaca behavior, habitat, diet, and farming practices, providing valuable insights for researchers and enthusiasts interested in these animals\n"
     ]
    }
   ],
   "source": [
    "output = text_generator(instruction_prompt(\"Tell me about a Alapcas.\", sample.get('input', None)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
